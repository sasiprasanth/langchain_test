{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04816460-dc87-4cb4-9376-6455456a030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb4fb864-5cc8-4535-83e3-398b3ac5b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc018965-4513-498f-9305-6a97516fdd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sasiprasanth/miniconda3/envs/langchain/lib/python3.9/site-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "# InstructorEmbedding \n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6371b9b9-3782-4017-af0d-132723744220",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f448810f-e46e-4e54-b6de-c2530dcbcfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = TextLoader('single_text_file.txt')\n",
    "loader = DirectoryLoader(f'./reports/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49b69382-3f65-400b-bcb2-8ec335e352fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'reports/2023_GPT4All-J_Technical_Report_2.pdf', 'page': 0},\n",
       " {'source': 'reports/2023_GPT4All-J_Technical_Report_2.pdf', 'page': 1},\n",
       " {'source': 'reports/2023_GPT4All-J_Technical_Report_2.pdf', 'page': 2},\n",
       " {'source': 'reports/2023_GPT4All_Technical_Report.pdf', 'page': 0},\n",
       " {'source': 'reports/2023_GPT4All_Technical_Report.pdf', 'page': 1},\n",
       " {'source': 'reports/2023_GPT4All_Technical_Report.pdf', 'page': 2}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[el.metadata for el in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6242f057-60a4-4bc3-a4e4-bcd59906a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "                                               chunk_size=1000, \n",
    "                                               chunk_overlap=200)\n",
    "\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "299553e6-bd9e-4e0d-aff3-aa94f7881462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='GPT4All-J: An Apache-2 Licensed Assistant-Style Chatbot\\nYuvanesh Anand\\nyuvanesh@nomic.aiZach Nussbaum\\nzach@nomic.aiBrandon Duderstadt\\nbrandon@nomic.ai\\nBenjamin M. Schmidt\\nben@nomic.aiAdam Treat\\ntreat.adam@gmail.comAndriy Mulyar\\nandriy@nomic.ai\\nAbstract\\nGPT4All-J is an Apache-2 licensed chatbot\\ntrained over a massive curated corpus of as-\\nsistant interactions including word problems,\\nmulti-turn dialogue, code, poems, songs, and\\nstories. It builds on the March 2023 GPT4All\\nrelease by training on a significantly larger\\ncorpus, by deriving its weights from the\\nApache-licensed GPT-J model rather than the\\nGPL-licensed of LLaMA, and by demonstrat-\\ning improved performance on creative tasks\\nsuch as writing stories, poems, songs and\\nplays. We openly release the training data,\\ndata curation procedure, training code, and fi-\\nnal model weights to promote open research\\nand reproducibility. Additionally, we release\\nPython bindings and a Chat UI to a quantized', metadata={'source': 'reports/2023_GPT4All-J_Technical_Report_2.pdf', 'page': 0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d37fa3b-2e6c-43c0-9ccd-32cb893bf344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66473d84-6205-454d-a8fd-9a2e2da48565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "847b53de-bf6f-4eff-9375-c664199f2490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import faiss\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95ae55cd-3ecf-4154-9b41-f55a5c4ad223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_embeddings(docs, embeddings, sotre_name, path):\n",
    "    \n",
    "    vectorStore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "    with open(f\"{path}/faiss_{sotre_name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(vectorStore, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a88ddae9-15a8-4bfe-aadc-f7ff35022b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(sotre_name, path):\n",
    "    with open(f\"{path}/faiss_{sotre_name}.pkl\", \"rb\") as f:\n",
    "        VectorStore = pickle.load(f)\n",
    "    return VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5a69d05-c93d-4b16-b329-1c78954ad25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", \n",
    "                                                      model_kwargs={\"device\": \"mps\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d797683-406f-4e91-8803-ceaa140d0489",
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding_store_path = f\"{root_dir}/Embedding_store\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f1d2ee1-0083-417c-b648-529074856f50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sasiprasanth/miniconda3/envs/langchain/lib/python3.9/site-packages/InstructorEmbedding/instructor.py:278: UserWarning: MPS: no support for int64 reduction ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:144.)\n",
      "  assert torch.sum(attention_mask[local_idx]).item() >= context_masks[local_idx].item(),\\\n"
     ]
    }
   ],
   "source": [
    "db_instructEmbedd = FAISS.from_documents(texts[:1], instructor_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1585149c-e813-4912-a899-23cc723fd872",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db_instructEmbedd.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "453de483-f8ca-4d7f-999c-ed8535ae5569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'similarity'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e9c5d33-1730-4bc6-9df4-37f623d11135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k': 3}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48b63fc6-ebea-4430-beff-57ea20eb77db",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"Who are the authors of GPT4All report?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb79b737-b3af-4a35-a572-70d2ca905b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='GPT4All-J: An Apache-2 Licensed Assistant-Style Chatbot\\nYuvanesh Anand\\nyuvanesh@nomic.aiZach Nussbaum\\nzach@nomic.aiBrandon Duderstadt\\nbrandon@nomic.ai\\nBenjamin M. Schmidt\\nben@nomic.aiAdam Treat\\ntreat.adam@gmail.comAndriy Mulyar\\nandriy@nomic.ai\\nAbstract\\nGPT4All-J is an Apache-2 licensed chatbot\\ntrained over a massive curated corpus of as-\\nsistant interactions including word problems,\\nmulti-turn dialogue, code, poems, songs, and\\nstories. It builds on the March 2023 GPT4All\\nrelease by training on a significantly larger\\ncorpus, by deriving its weights from the\\nApache-licensed GPT-J model rather than the\\nGPL-licensed of LLaMA, and by demonstrat-\\ning improved performance on creative tasks\\nsuch as writing stories, poems, songs and\\nplays. We openly release the training data,\\ndata curation procedure, training code, and fi-\\nnal model weights to promote open research\\nand reproducibility. Additionally, we release\\nPython bindings and a Chat UI to a quantized', metadata={'source': 'reports/2023_GPT4All-J_Technical_Report_2.pdf', 'page': 0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3484d-0bd3-4282-9549-6e38d7cb6226",
   "metadata": {},
   "source": [
    "## Creating LLama2 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88cc3d0c-35b1-40e9-a091-4733a0e5bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aab7bef2-ade4-440c-b339-2475b2295f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../llama-2-7b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 3917.73 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/sasiprasanth/miniconda3/envs/langchain/lib/python3.9/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x431508fa0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x43150bbd0\n",
      "ggml_metal_init: loaded kernel_mul                            0x43150bee0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x43150c1f0\n",
      "ggml_metal_init: loaded kernel_scale                          0x43150c500\n",
      "ggml_metal_init: loaded kernel_silu                           0x43150c810\n",
      "ggml_metal_init: loaded kernel_relu                           0x43150cb20\n",
      "ggml_metal_init: loaded kernel_gelu                           0x43150ce30\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x43150d2d0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x43150d720\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x43150db90\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x43150e000\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x43150e470\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x43150e8e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x43150eea0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x43150f310\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x43150f780\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x43150fbf0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x4315100a0\n",
      "ggml_metal_init: loaded kernel_norm                           0x431510540\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x431510b90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x431511040\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x4315114f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x4315119a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x431511e50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x431512300\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x431512790\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x431512c20\n",
      "ggml_metal_init: loaded kernel_rope                           0x431512f30\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x4315135e0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x431513c60\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x4315142e0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x4315145f0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 10922.67 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =    70.31 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3616.08 MB, ( 8920.39 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.00 MB, ( 8930.39 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 9188.39 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, ( 9320.39 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, ( 9480.39 / 10922.67)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"../llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdae024-9d2f-426d-9f2d-66b95182f8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12124499-7d6c-4de1-944b-30a0c01eb923",
   "metadata": {},
   "source": [
    "## Creting LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dda8d39-c446-4fda-898f-8d033d5d5762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the chain to answer questions \n",
    "qa_chain_instrucEmbed = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                  chain_type=\"stuff\", \n",
    "                                  retriever=retriever, \n",
    "                                  return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83663474-5246-4ed2-b93e-35bdf979f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cite sources\n",
    "\n",
    "import textwrap\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
    "    print('\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "597b5bbb-0678-4233-aeb5-b787f3cb5f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Instructor Embeddings------------------\n",
      "\n",
      " The authors of the GPT4All technical report are Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin M. Schmidt, Adam Treat, and Andriy Mulyar. The authors of the GPT4All technical report are Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin\n",
      "M. Schmidt, Adam Treat, and Andriy Mulyar.\n",
      "\n",
      "Sources:\n",
      "reports/2023_GPT4All-J_Technical_Report_2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 13129.63 ms\n",
      "llama_print_timings:      sample time =    32.37 ms /    45 runs   (    0.72 ms per token,  1390.00 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13129.58 ms /   365 tokens (   35.97 ms per token,    27.80 tokens per second)\n",
      "llama_print_timings:        eval time =  5153.40 ms /    44 runs   (  117.12 ms per token,     8.54 tokens per second)\n",
      "llama_print_timings:       total time = 18381.38 ms\n"
     ]
    }
   ],
   "source": [
    "query = 'who are the authors of GPT4all technical report?'\n",
    "\n",
    "print('-------------------Instructor Embeddings------------------\\n')\n",
    "llm_response = qa_chain_instrucEmbed(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e00c9b77-c340-4fa0-8dae-318d66c4513d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Instructor Embeddings------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The topic of the report is the GPT4All-J chatbot, specifically its features, capabilities, and how it was trained. The topic of the report is the GPT4All-J chatbot, specifically its features, capabilities, and how it was\n",
      "trained.\n",
      "\n",
      "Sources:\n",
      "reports/2023_GPT4All-J_Technical_Report_2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 13129.63 ms\n",
      "llama_print_timings:      sample time =    22.25 ms /    29 runs   (    0.77 ms per token,  1303.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =   520.26 ms /    13 tokens (   40.02 ms per token,    24.99 tokens per second)\n",
      "llama_print_timings:        eval time =   993.83 ms /    28 runs   (   35.49 ms per token,    28.17 tokens per second)\n",
      "llama_print_timings:       total time =  1992.34 ms\n"
     ]
    }
   ],
   "source": [
    "query = 'What is the topic of the report?'\n",
    "\n",
    "print('-------------------Instructor Embeddings------------------\\n')\n",
    "llm_response = qa_chain_instrucEmbed(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f07959-a150-4d81-8667-d9cf8f1b7680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a25b080-98c5-4839-bd6e-3d8411e6180a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
